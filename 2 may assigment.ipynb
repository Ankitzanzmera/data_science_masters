{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "235a888a",
   "metadata": {},
   "source": [
    "#### Q1. What is anomaly detection and what is its purpose?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf3ac40",
   "metadata": {},
   "source": [
    "Anomaly detection is a technique used in data analysis to identify patterns that deviate from the expected or normal behavior. It is an observation that differs significantly from the majority of the data points.\n",
    "\n",
    "The purpose of anomaly detection is to identify unusual or unexpected events or behaviors in data that may indicate a problem, error, fraud, or any other abnormality. Anomaly detection is used in a variety of applications, including:\n",
    "\n",
    "Cybersecurity: detecting unusual network traffic that may indicate a security breach or an attack.\n",
    "    \n",
    "Fraud detection: identifying fraudulent transactions, such as credit card fraud, insurance fraud, or money laundering.\n",
    "    \n",
    "Health monitoring: detecting abnormalities in medical data, like abnormal heart rhythms or glucose levels, which may indicate a health issue.\n",
    "Manufacturing: detecting defects or anomalies in products during the manufacturing process.\n",
    "    \n",
    "Predictive maintenance: identifying unusual patterns in machine data to detect potential faults or failures before they occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3197f818",
   "metadata": {},
   "source": [
    "#### Q2. What are the key challenges in anomaly detection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58555c02",
   "metadata": {},
   "source": [
    "Lack of labeled data: In many cases, anomaly detection requires labeled data to train models accurately. However, in real-world scenarios, labeled data is often scarce, which makes it challenging to build robust anomaly detection models.\n",
    "\n",
    "Class imbalance: Anomaly detection problems often suffer from class imbalance, meaning that the number of anomalous examples is significantly smaller than the number of normal examples. This can make it difficult to train models that can accurately identify anomalies.\n",
    "\n",
    "Data quality: Anomaly detection models rely heavily on the quality of the input data. If the data is noisy, incomplete, or contains errors, it can affect the accuracy of the model.\n",
    "\n",
    "High-dimensional data: In some cases, the data used for anomaly detection may be high-dimensional, which can make it challenging to identify relevant features that distinguish normal and anomalous behavior.\n",
    "\n",
    "Changing data patterns: Anomaly detection models must be able to adapt to changes in data patterns over time. For example, what was considered anomalous behavior in the past may no longer be anomalous in the present, or new types of anomalies may emerge.\n",
    "\n",
    "Interpretability: Finally, anomaly detection models may be challenging to interpret, which can make it difficult to understand why a particular data point was flagged as anomalous. This can limit the usefulness of the model for decision-making purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8d83df",
   "metadata": {},
   "source": [
    "#### Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fbed47",
   "metadata": {},
   "source": [
    "Unsupervised and supervised anomaly detection are two main approaches used to identify anomalous data points in a dataset, and they differ in the type of input data and the learning process.\n",
    "\n",
    "Supervised anomaly detection:\n",
    "\n",
    "Input data: Supervised anomaly detection requires labeled data, where each data point is labeled as normal or anomalous.\n",
    "Learning process: A supervised model is trained on the labeled data to identify the patterns and features that distinguish normal and anomalous data points. The model uses this information to predict whether new, unlabeled data points are normal or anomalous.\n",
    "Unsupervised anomaly detection:\n",
    "\n",
    "Input data: Unsupervised anomaly detection does not require labeled data. Instead, it works by identifying data points that are significantly different from the rest of the data, without any prior knowledge about what constitutes normal or anomalous behavior.\n",
    "Learning process: An unsupervised model is trained to learn the patterns and features that are present in the data, and then identify data points that deviate significantly from those patterns. This can be useful when the data has no labeled examples or when the anomalies are not well-defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40739943",
   "metadata": {},
   "source": [
    "#### Q4. What are the main categories of anomaly detection algorithms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d2bf2",
   "metadata": {},
   "source": [
    "Statistical methods: Statistical methods assume that the data follows a certain statistical distribution, such as Gaussian or Poisson. These methods identify anomalies as data points that fall outside of a certain range of expected values or have a low probability of occurring based on the assumed distribution.\n",
    "\n",
    "Machine learning methods: Machine learning methods use algorithms such as clustering, classification, and regression to identify anomalies based on patterns in the data. These methods can be supervised or unsupervised, and they often require labeled data for training.\n",
    "\n",
    "Time-series methods: Time-series methods are designed for data that is collected over time, such as sensor readings or stock prices. These methods look for changes in the time-series data that may indicate anomalies, such as sudden spikes or drops.\n",
    "Deep learning methods: Deep learning methods, such as autoencoders and recurrent neural networks, can learn complex patterns in the data and identify anomalies based on deviations from those patterns.\n",
    "\n",
    "Information theory-based methods: Information theory-based methods use measures such as entropy and mutual information to detect anomalies by identifying unexpected changes in the data.\n",
    "\n",
    "Domain-specific methods: Domain-specific methods are tailored to specific applications or domains, such as image or audio processing, and use specialized algorithms to identify anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca39832f",
   "metadata": {},
   "source": [
    "#### Q5. What are the main assumptions made by distance-based anomaly detection methods?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef035e69",
   "metadata": {},
   "source": [
    "Normal data points are densely clustered: Distance-based methods assume that normal data points are clustered closely together in the feature space. Therefore, any data point that is far away from the normal cluster is considered an anomaly.\n",
    "\n",
    "Anomalous data points are isolated: Anomalous data points are assumed to be isolated and located far away from the normal cluster. This is based on the intuition that anomalies are rare and unusual, and therefore unlikely to be clustered together with normal data points.\n",
    "\n",
    "Distance or similarity measures capture meaningful differences between data points: Distance-based methods rely on a distance or similarity measure to quantify the difference between data points. These measures are assumed to capture meaningful differences between data points, such that anomalous data points have significantly higher distances or dissimilarities than normal data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871676fa",
   "metadata": {},
   "source": [
    "#### Q6. How does the LOF algorithm compute anomaly scores?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d0b78d",
   "metadata": {},
   "source": [
    "For each data point, the k nearest neighbors are identified based on a distance or similarity measure.\n",
    "\n",
    "The local reachability density (LRD) of the data point is then computed as the inverse of the average reachability distance of its k nearest neighbors. The reachability distance is a measure of the distance between two data points that takes into account the density of the data points around them.\n",
    "\n",
    "The local outlier factor (LOF) of the data point is then computed as the ratio of the LRD of the data point to the average LRD of its k nearest neighbors. The LOF measures the degree to which a data point is an outlier compared to its neighbors.\n",
    "The local outlier factor (LOF) scores can be normalized to a range between 0 and 1, where a score of 1 indicates a highly anomalous data point and a score of 0 indicates a normal data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf43c34c",
   "metadata": {},
   "source": [
    "#### Q7. What are the key parameters of the Isolation Forest algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4598f7",
   "metadata": {},
   "source": [
    "n_estimators: This parameter determines the number of isolation trees in the forest. A higher number of trees can improve the accuracy of the anomaly detection, but also increase the computational cost.\n",
    "\n",
    "max_samples: This parameter determines the number of samples used to build each isolation tree. A smaller number of samples can increase the diversity of the trees and improve the accuracy of the anomaly detection, but also reduce the efficiency of the algorithm.\n",
    "\n",
    "contamination: This parameter determines the expected proportion of anomalies in the dataset. It is used to set a threshold for identifying anomalies based on the anomaly score computed by the Isolation Forest algorithm to detect some anomalies or outliers.\n",
    "\n",
    "max_features: This parameter determines the number of features used to split each node in the isolation tree. A smaller number of features can increase the diversity of the trees and improve the accuracy of the anomaly detection, but also reduce the effectiveness of the algorithm.\n",
    "\n",
    "random_state: This parameter determines the random seed used to initialize the random number generator, which can affect the reproducibility of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209f37f",
   "metadata": {},
   "source": [
    "#### Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a3065a",
   "metadata": {},
   "source": [
    "To compute the anomaly score of a data point using KNN with K=10, we need to compare its distance to its 10th nearest neighbor with the distances of its K nearest neighbors. If the distance is much larger than the distances of its K nearest neighbors, then the data point is considered an anomaly.\n",
    "\n",
    "In this case, the data point has only 2 neighbors of the same class within a radius of 0.5, which means that it has less than 10 neighbors in total. Therefore, we can't compute the anomaly score using KNN with K=10 for this data point.\n",
    "\n",
    "Instead, we can use a smaller value of K, such as K=2 or K=3, to compute the anomaly score. For example, if we use K=2 and the distance to the 2nd nearest neighbor is much larger than the distance to the 1st nearest neighbor, then the data point is considered an anomaly. However, if the distances to the 1st and 2nd nearest neighbors are similar, then the data point is likely not an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be8d590",
   "metadata": {},
   "source": [
    "#### Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b41e9ac",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm assigns an anomaly score to each data point based on its average path length in a forest of isolation trees. The average path length is a measure of how many edges are traversed to isolate a data point in a tree. The anomaly score is computed as the inverse of the average path length, normalized to a range between 0 and 1.\n",
    "\n",
    "Given that we have 100 trees in the forest and a dataset of 3000 data points, the average path length for a normal data point can be approximated, or it can be calculated as follows:\n",
    "\n",
    "In each tree, a data point is isolated by traversing an average of log2(n) edges, where n is the number of data points in the tree. For 3000 data points, log2(3000) is approximately 11.\n",
    "Therefore, the average path length for a normal data point in a single tree is approximately 11.\n",
    "The average path length for a normal data point in the entire forest can be estimated as the average of the path lengths in all 100 trees, which is approximately 11.\n",
    "If a data point has an average path length significantly less than 11, then it is considered an anomaly.\n",
    "In this case, the data point has an average path length of 5.0 compared to the average path length of the trees. Therefore, its anomaly score can be computed as follows:\n",
    "\n",
    "The inverse of the average path length is 1/5.0 = 0.2.\n",
    "The anomaly score is then normalized to a range between 0 and 1 by subtracting it from 1, which gives a score of 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50073b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
