{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe1ef50c",
   "metadata": {},
   "source": [
    "#### Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc7ca3",
   "metadata": {},
   "source": [
    "__Missing Data that's Missing Completely at Random (MCAR)__ : \n",
    "These are data that are missing completely at random. That is, the missingness is independent from the data. There is no discernible pattern to this type of data missingness.\n",
    "\n",
    "__Missing Data that's Missing at Random (MAR)__ :\n",
    "These types of data are missing at random but not completely missing. The data's missingness is determined by the data you see.\n",
    "\n",
    "__Missing Data that's Not Missing at Random (NMAR)__:\n",
    "These are data that are not missing at random and are also known as ignorable data. In other words, the missingness of the missing data is determined by the variable of interest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028a45d7",
   "metadata": {},
   "source": [
    "__Why is it essential to handle missing values__ :\n",
    "    Missing data are problematic because, depending on the type, they can sometimes bias your results. This means your results may not be generalisable outside of your study because your data come from an unrepresentative sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6d90a8",
   "metadata": {},
   "source": [
    "#### Q2: List down techniques used to handle missing data. Give an example of each with python code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb7f5c6",
   "metadata": {},
   "source": [
    "1. Delete Rows with Missing Values\n",
    "2. Impute missing values with Mean/Median:\n",
    "3. Imputation method for categorical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29111582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>Second</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass     sex   age  sibsp  parch     fare embarked   class  \\\n",
       "0           0       3    male  22.0      1      0   7.2500        S   Third   \n",
       "1           1       1  female  38.0      1      0  71.2833        C   First   \n",
       "2           1       3  female  26.0      0      0   7.9250        S   Third   \n",
       "3           1       1  female  35.0      1      0  53.1000        S   First   \n",
       "4           0       3    male  35.0      0      0   8.0500        S   Third   \n",
       "..        ...     ...     ...   ...    ...    ...      ...      ...     ...   \n",
       "886         0       2    male  27.0      0      0  13.0000        S  Second   \n",
       "887         1       1  female  19.0      0      0  30.0000        S   First   \n",
       "888         0       3  female   NaN      1      2  23.4500        S   Third   \n",
       "889         1       1    male  26.0      0      0  30.0000        C   First   \n",
       "890         0       3    male  32.0      0      0   7.7500        Q   Third   \n",
       "\n",
       "       who  adult_male deck  embark_town alive  alone  \n",
       "0      man        True  NaN  Southampton    no  False  \n",
       "1    woman       False    C    Cherbourg   yes  False  \n",
       "2    woman       False  NaN  Southampton   yes   True  \n",
       "3    woman       False    C  Southampton   yes  False  \n",
       "4      man        True  NaN  Southampton    no   True  \n",
       "..     ...         ...  ...          ...   ...    ...  \n",
       "886    man        True  NaN  Southampton    no   True  \n",
       "887  woman       False    B  Southampton   yes   True  \n",
       "888  woman       False  NaN  Southampton    no  False  \n",
       "889    man        True    C    Cherbourg   yes   True  \n",
       "890    man        True  NaN   Queenstown    no   True  \n",
       "\n",
       "[891 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "df = sns.load_dataset('titanic')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9be6ac8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      S\n",
       "1      C\n",
       "2      S\n",
       "3      S\n",
       "4      S\n",
       "      ..\n",
       "886    S\n",
       "887    S\n",
       "888    S\n",
       "889    C\n",
       "890    Q\n",
       "Name: embarked, Length: 891, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete rows\n",
    "df.dropna()\n",
    "\n",
    "# impute missing value mean/median\n",
    "df['age'].fillna(df['age'].mean())\n",
    "\n",
    "# impute missing categorial\n",
    "mode = df[df['embarked'].notna()]['embarked'].mode()[0]\n",
    "df['embarked'].fillna(mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab373379",
   "metadata": {},
   "source": [
    "#### Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4288838c",
   "metadata": {},
   "source": [
    "An imbalanced dataset means instances of one of the two classes is higher than the other, in another way, the number of observations is not the same for all the classes in a classification dataset.\n",
    "\n",
    "This problem is faced not only in the binary class data but also in the multi-class data.\n",
    "\n",
    "The problem with training the model with an imbalanced dataset is that the model will be biased towards the majority class only. This causes a problem when we are interested in the prediction of the minority class \\\n",
    "\n",
    "if dataset is imbalance then model will bias toward the one result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e26ffa",
   "metadata": {},
   "source": [
    "#### Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c6aa80",
   "metadata": {},
   "source": [
    "Downsampling (in this context) means training on a disproportionately low subset of the majority class examples.\n",
    "\n",
    "Upweighting means adding an example weight to the downsampled class equal to the factor by which you downsampled.\n",
    "\n",
    "If there are two classes, then balanced data would mean 50% points for each of the class. For most machine learning techniques, little imbalance is not a problem. So, if there are 60% points for one class and 40% for the other class, it should not cause any significant performance degradation. Only when the class imbalance is high, e.g. 90% points for one class and 10% for the other, standard optimization criteria or performance measures may not be as effective and would need modification.\n",
    "\n",
    "A typical example of imbalanced data is encountered in e-mail classification problem where emails are classified into ham or spam. The number of spam emails is usually lower than the number of relevant (ham) emails. So, using the original distribution of two classes leads to imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f5f8af",
   "metadata": {},
   "source": [
    "#### Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1147ae",
   "metadata": {},
   "source": [
    "Data augmentation is a set of techniques to artificially increase the amount of data by generating new data points from existing data. This includes making small changes to data or using deep learning models to generate new data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad7a37",
   "metadata": {},
   "source": [
    "__SMOTE__ : \n",
    "    Synthetic Minority Oversampling Technique (SMOTE) is a statistical technique for increasing the number of cases in your dataset in a balanced way. The component works by generating new instances from existing minority cases that you supply as input. This implementation of SMOTE does not change the number of majority cases.\n",
    "\n",
    "The new instances are not just copies of existing minority cases. Instead, the algorithm takes samples of the feature space for each target class and its nearest neighbors. The algorithm then generates new examples that combine features of the target case with features of its neighbors. This approach increases the features available to each class and makes the samples more general.\n",
    "\n",
    "SMOTE takes the entire dataset as an input, but it increases the percentage of only the minority cases. For example, suppose you have an imbalanced dataset where just 1 percent of the cases have the target value A (the minority class), and 99 percent of the cases have the value B. To increase the percentage of minority cases to twice the previous percentage, you would enter 200 for SMOTE percentage in the component's properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2a5ad5",
   "metadata": {},
   "source": [
    "#### Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb0844e",
   "metadata": {},
   "source": [
    "An outlier is an observation that lies an abnormal distance from other values in a random sample from a population. In a sense, this definition leaves it up to the analyst (or a consensus process) to decide what will be considered abnormal. Before abnormal observations can be singled out, it is necessary to characterize normal observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b28596",
   "metadata": {},
   "source": [
    "Outliers are important because they can have a large influence on statistics derived from the dataset. For example, the mean intake of energy or some nutrient may be [glossary term:]skewed upward or downward by one or a few extreme values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40faae17",
   "metadata": {},
   "source": [
    "#### Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d11cad8",
   "metadata": {},
   "source": [
    "If there Most of of value is Missing in any partucular column then i will delete that column. if column is important for model than i can impute mean of column if there is no outlier in column otherwise i may use median. if mising value is present in  categorial column then i will put mode value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36344735",
   "metadata": {},
   "source": [
    "#### Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f69887",
   "metadata": {},
   "source": [
    "i may impute missing data with mean or median "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323c8be9",
   "metadata": {},
   "source": [
    "#### Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c3515",
   "metadata": {},
   "source": [
    "i will upsample theminority class to provide more data of miority class to model so that model can work better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3804507",
   "metadata": {},
   "source": [
    "#### Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1627efa",
   "metadata": {},
   "source": [
    "i shouldn't down sample the majority class becuase if i down sample the class it most likely than important data will be lost.so think i should upsample the customer unsatisfied reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac425b91",
   "metadata": {},
   "source": [
    "#### Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bd0438",
   "metadata": {},
   "source": [
    "yes i should up sample the minority class. becuase if we up sample the rare event then model get higher instance and can work better on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b74355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
